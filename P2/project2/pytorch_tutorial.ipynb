{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EECS 445 PyTorch Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from the `EECS 498 - Deep Learning` tutorial found on ( https://github.com/honglaklee/deep-learning-course/blob/master/CodeExamples/01_basics.ipynb )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "WHAT IS PYTORCH?\n",
    "\n",
    "It’s a Python-based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "* A replacement for NumPy to use the power of GPUs\n",
    "* a deep learning research platform that provides maximum flexibility and speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch provides functions similar to numpy to create tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 3.6893e+19, 0.0000e+00],\n",
      "        [3.6893e+19, 5.6052e-45, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [3.6893e+19, 3.8246e-34, 8.5920e+09],\n",
      "        [2.8026e-45, 3.6893e+19, 3.8226e-34]])\n",
      "torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.empty(5, 3) # Construct a 5x3 matrix, uninitialized\n",
    "print(x)\n",
    "print(x.size()) # torch.Size is a tuple, so it supports all tuple operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0207, 0.8633, 0.0724],\n",
      "        [0.2607, 0.4335, 0.4868]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 3) # Construct a randomly initialized matrix\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0],\n",
      "        [0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros(5, 3, dtype=torch.long) # Construct a matrix filled zeros and of dtype long\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.5000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([5.5, 3]) # Construct a tensor directly from data, similarly to how you construct NumPy Arrays\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "tensor([[ 1.4561,  0.1384,  1.4581],\n",
      "        [-0.3650,  0.6728, -1.1754]])\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor based on an existing tensor.\n",
    "# These methods will reuse properties of the input tensor, e.g. dtype, unless new values are provided by user\n",
    "x = x.new_ones(2, 3)      # new_* methods take in sizes\n",
    "x = x.new_ones((2, 3), dtype=torch.double)      # this is equivalent\n",
    "print(x)\n",
    "\n",
    "x = torch.randn_like(x, dtype=torch.float)    # override dtype!\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are multiple syntaxes for operations. In the following example, we will take a look at the addition operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.0091,  0.8896,  2.2819],\n",
      "        [-0.1125,  1.3262, -0.3386]])\n",
      "tensor([[ 2.0091,  0.8896,  2.2819],\n",
      "        [-0.1125,  1.3262, -0.3386]])\n",
      "tensor([[ 2.0091,  0.8896,  2.2819],\n",
      "        [-0.1125,  1.3262, -0.3386]])\n",
      "tensor([[ 2.0091,  0.8896,  2.2819],\n",
      "        [-0.1125,  1.3262, -0.3386]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.rand(2, 3)\n",
    "print(x + y)\n",
    "\n",
    "print(torch.add(x, y))\n",
    "\n",
    "result = torch.empty(2, 3)\n",
    "torch.add(x, y, out=result) # Providing an output tensor as argument\n",
    "print(result)\n",
    "\n",
    "y.add_(x) # In-place addition\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Any operation that mutates a tensor in-place is post-fixed with an \\_. For example: x.copy_(y), x.t_(), will change x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "You can use standard numpy-like indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a one element tensor, use .item() to get the value as a Python number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.0010])\n",
      "-1.00099778175354\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read later:**\n",
    "\n",
    "\n",
    "  100+ Tensor operations, including transposing, indexing, slicing,\n",
    "  mathematical operations, linear algebra, random numbers, etc.,\n",
    "  are described here <https://pytorch.org/docs/torch>.\n",
    "\n",
    "### NumPy Bridge\n",
    "\n",
    "Converting a Torch Tensor to a NumPy array and vice versa is a breeze.\n",
    "\n",
    "**Important**: The Torch Tensor and NumPy array will share their underlying memory\n",
    "locations, and changing one will change the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "[1. 1. 1. 1. 1.]\n",
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)\n",
    "b = a.numpy() # Converts torch tensor to numpy array\n",
    "print(b)\n",
    "\n",
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a) # Converts numpy array to torch tensor\n",
    "np.add(a, 1, out=a) # broadcasting\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA Tensor\n",
    "Tensors can be moved onto any device using the .to method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.nn is a Neural Network package useful for constructing models which provides layers, activation functions, loss functions, etc. ( https://github.com/torch/nn )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a polynomial function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "POLY_DEGREE = 4\n",
    "W_target = torch.randn(POLY_DEGREE, 1) * 5\n",
    "b_target = torch.randn(1) * 5\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"Approximated function.\"\"\"\n",
    "    return x.mm(W_target) + b_target.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data loader and other utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_features(x):\n",
    "    \"\"\"Builds features i.e. a matrix with columns [x, x^2, x^3, x^4].\"\"\"\n",
    "    # change shape of x from (N,) to (N,1)\n",
    "    x = x.unsqueeze(1)\n",
    "    # create POLY_DEGREE (N,1) tensors and concatenate them across dimension 1 into 1 (N, POLY_DEGREE) tensor\n",
    "    return torch.cat([x ** i for i in range(1, POLY_DEGREE+1)], 1)\n",
    "\n",
    "def get_batch(batch_size=32):\n",
    "    \"\"\"Builds a batch i.e. (x, f(x)) pair.\"\"\"\n",
    "    random = torch.randn(batch_size)\n",
    "    x = make_features(random)\n",
    "    y = f(x)\n",
    "    return x, y\n",
    "\n",
    "def poly_desc(W, b):\n",
    "    \"\"\"Creates a string description of a polynomial.\"\"\"\n",
    "    result = 'y = '\n",
    "    for i, w in enumerate(W):\n",
    "        result += '{:+.2f} x^{} '.format(w, len(W) - i)\n",
    "    result += '{:+.2f}'.format(b[0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class myNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(W_target.size(0), 1)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        nn.init.normal_(self.fc.weight, 0.0, 1 / np.sqrt(W_target.size(0)))\n",
    "        nn.init.constant_(self.fc.bias, 0.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, POLY_DEGREE = x.shape\n",
    "        z = self.fc(x)\n",
    "        h = z.clone()  # normally you would apply some activation function here\n",
    "        return h\n",
    "\n",
    "    \n",
    "model = myNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.000881 after 378 batches\n",
      "==> Learned function:\ty = +8.82 x^4 -1.26 x^3 -4.52 x^2 -1.83 x^1 -5.72\n",
      "==> Actual function:\ty = +8.85 x^4 -1.26 x^3 -4.52 x^2 -1.83 x^1 -5.71\n"
     ]
    }
   ],
   "source": [
    "for batch_idx in count(1):\n",
    "    # Get data\n",
    "    batch_x, batch_y = get_batch()\n",
    "\n",
    "    # Reset gradients\n",
    "    # This is necessary because otherwise gradients would accumulate, and end results would be skewed\n",
    "    model.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = F.smooth_l1_loss(model(batch_x), batch_y)\n",
    "    loss = output.item()\n",
    "\n",
    "    # Backward pass using backpropagation of the gradient\n",
    "    output.backward()\n",
    "    \n",
    "    # Optimize weights using SGD\n",
    "    for param in model.parameters():\n",
    "        param.data.add_(-0.1 * param.grad.data)\n",
    "\n",
    "    # Stop criterion\n",
    "    if loss < 1e-3:\n",
    "        break\n",
    "\n",
    "print('Loss: {:.6f} after {} batches'.format(loss, batch_idx))\n",
    "print('==> Learned function:\\t' + poly_desc(model.fc.weight.view(-1), model.fc.bias))\n",
    "print('==> Actual function:\\t' + poly_desc(W_target.view(-1), b_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a slightly more advanced example of a Convolutional Neural Network for MNIST digit classification. <https://github.com/pytorch/examples/blob/master/mnist/main.py>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Resources\n",
    "\n",
    "Here are some other useful resources on PyTorch\n",
    "\n",
    "* https://cs230-stanford.github.io/pytorch-getting-started.html\n",
    "* https://github.com/jcjohnson/pytorch-examples\n",
    "* https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
